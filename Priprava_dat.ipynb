{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Czech Parliament Meetings (CPM) transcriptions - data preprocessing\n",
    "The CPM web site contains a huge amount of transcriptions from all meetings. The kinds we're interested in are [here](http://www.psp.cz/eknih/2013ps/stenprot/index.htm). Fortunately, we don't have to dowload all individual texts, because there are also transcriptions in a compressed format [here](http://www.psp.cz/eknih/2013ps/stenprot/zip/index.htm).\n",
    "\n",
    "The workflow is: download ZIPs, extract them, parse them into json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as ur\n",
    "import urllib.parse as up\n",
    "import urllib\n",
    "import lxml.html\n",
    "import os.path\n",
    "import zipfile\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'http://www.psp.cz/eknih/'\n",
    "zp_folder = './zip/'\n",
    "zp_ext = '.zip'\n",
    "st_folder = './html/'\n",
    "js_folder = 'json/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a page with a list of the meetings and look for the links to Parliament meetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = lxml.html.parse(base).getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.psp.cz/eknih/2017ps/index.htm', 'http://www.psp.cz/eknih/2013ps/index.htm', 'http://www.psp.cz/eknih/2010ps/index.htm', 'http://www.psp.cz/eknih/2006ps/index.htm', 'http://www.psp.cz/eknih/2002ps/index.htm', 'http://www.psp.cz/eknih/1998ps/index.htm', 'http://www.psp.cz/eknih/1996ps/index.htm']\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "\n",
    "libs = ht.cssselect('div#main-content')[0]\n",
    "for elem_a, elem_b in zip(libs.cssselect('a'), libs.cssselect('a b')):\n",
    "    if elem_b.text.endswith('Poslanecká sněmovna'):\n",
    "        links.append(up.urljoin(base, elem_a.attrib['href']))\n",
    "\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting links to compressed files from individual meetings web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.psp.cz/eknih/2017ps/stenprot/zip/', 'http://www.psp.cz/eknih/2013ps/stenprot/zip/index.htm', 'http://www.psp.cz/eknih/2010ps/stenprot/zip/index.htm', 'http://www.psp.cz/eknih/2006ps/stenprot/zip/index.htm', 'http://www.psp.cz/eknih/2002ps/stenprot/zip/index.htm']\n"
     ]
    }
   ],
   "source": [
    "comp_links = []\n",
    "\n",
    "for ln in links:\n",
    "    page = lxml.html.parse(ln).getroot().cssselect('div#main-content')[0]\n",
    "    for elem_a in page.cssselect('a'):\n",
    "        if elem_a.text:\n",
    "            if 'komprimované' in elem_a.text:\n",
    "                comp_links.append(up.urljoin(ln, elem_a.attrib['href']))\n",
    "\n",
    "print(comp_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each site with compressed files contains several of them. So we download the page, find all links to files and download those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://www.psp.cz/eknih/2017ps/stenprot/zip/037schuz.zip done.\n",
      "Retrieving http://www.psp.cz/eknih/2013ps/stenprot/zip/059schuz.zip failed: HTTP Error 404: Not Found\n",
      "Retrieving http://www.psp.cz/eknih/2013ps/stenprot/zip/060schuz.zip failed: HTTP Error 404: Not Found\n",
      "Retrieving http://www.psp.cz/eknih/2013ps/stenprot/zip/061schuz.zip failed: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('zip'): os.mkdir('zip')\n",
    "page_num = 0\n",
    "for comp_link in comp_links:\n",
    "    page = lxml.html.parse(comp_link).getroot()\n",
    "    for ln in page.cssselect('div#main-content a'):\n",
    "        ln_attr_href = ln.attrib['href'].split('/')[-1]\n",
    "        target = os.path.join(zp_folder, str(page_num) + '_' + ln_attr_href)\n",
    "        if os.path.isfile(target): continue\n",
    "        try:\n",
    "            print('Retrieving ' + up.urljoin(comp_link, ln_attr_href) + ' ', end='')\n",
    "            ur.urlretrieve(up.urljoin(comp_link, ln_attr_href), target)\n",
    "            print('done.')\n",
    "        except urllib.error.HTTPError as err:\n",
    "            print('failed: {}'.format(err))\n",
    "    page_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./zip/2_042schuz.zip to ./html/2_042schuz failed: [Errno 22] Invalid argument\n",
      "Extracting ./zip/0_037schuz.zip to ./html/0_037schuz done.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('html'): os.mkdir('html')\n",
    "\n",
    "for arch in os.listdir(zp_folder):\n",
    "    if arch.endswith(zp_ext):\n",
    "        file_name = os.path.join(zp_folder, arch)\n",
    "        target = os.path.join(st_folder, arch.split('.')[0])\n",
    "\n",
    "        if os.path.exists(target): continue\n",
    "        \n",
    "        with zipfile.ZipFile(file_name) as zpf:\n",
    "            print('Extracting ' + file_name + ' to ' + target + ' ', end='')\n",
    "            try:\n",
    "                zpf.extractall(target)\n",
    "                print('done.')\n",
    "            except OSError as err:\n",
    "                os.rmdir(target)\n",
    "                print('failed: {}'.format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to text. There are a few notes:\n",
    "\n",
    "- Each paragraph is either a follow up onto a speech, or the beginning of a new one.\n",
    "- The beginning is determined by a link to the speaker profile. However, it's risky to simply determine a beginning by a link because transcriptions used to contain links to votes.\n",
    "- Speeches may overflow one file and continue in another. It doesn't matter since one meeting is simply one flow on text, no matter the ends of files.\n",
    "- Speaker names include their (politic) position. This is problematic since one person can have multiple names. It is \"solved\" by having a list of possible name preffixes (role, position, term of address, etc.)\n",
    "- EDIT: As of October 2019, the preffix removal works well for all entries. A problem will occure when a politician's name is \"Poslanec\" (Member of Parliament) or \"Senátor\" (Senator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(js_folder): os.mkdir(js_folder)\n",
    "html_files = glob.glob('./html/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "poz = 'Poslanec; PSP; Paní; Senátorka; Senátor; \\\n",
    "Poslankyně; mužů; Předsedající; práv; republiky; financí; prostředí; věcí; ČR'.split('; ')\n",
    "\n",
    "def rm_position(name):    \n",
    "    for p in poz:\n",
    "        if p in name:\n",
    "            name = name[name.rindex(p) + len(p) + 1:]\n",
    "            \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./html/2_010schuz\n"
     ]
    }
   ],
   "source": [
    "def write_json(nm, dt):\n",
    "    fn = os.path.join(js_folder, '%s.json' % nm)\n",
    "    \n",
    "    with open(fn, 'w') as f:\n",
    "        t = json.dump(dt, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "res = []\n",
    "pid = 0\n",
    "aut = None\n",
    "tema = None\n",
    "buf = []\n",
    "for htmlf in html_files:\n",
    "    buff = []\n",
    "    fns = glob.glob(os.path.join(htmlf, 's*.htm'))\n",
    "    for fn in fns:\n",
    "        h = lxml.html.parse(fn).getroot()\n",
    "        for p in h.cssselect('p'):\n",
    "            pt = p.text_content().strip()\n",
    "            if len(pt) == 0: continue\n",
    "            pt = pt.replace('\\xa0', ' ')\n",
    "            \n",
    "            od = p.find('a') # v textu je odkaz\n",
    "            if od is None:\n",
    "                buf += [pt]\n",
    "                continue\n",
    "\n",
    "            if len(buf) > 0:\n",
    "                buff.extend([OrderedDict(id=pid, autor=aut, schuze=int(htmlf.split('/')[-1][:3]),\\\n",
    "                                         fn=fn, tema=tema, text='\\n'.join(buf))])\n",
    "\n",
    "            aut = rm_position(od.text.strip())\n",
    "            buf = [pt[len(od.text)+1:].strip()] # pridame soucasny text (ale odseknem autora)\n",
    "            pid += 1\n",
    "    write_json(htmlf[-htmlf[::-1].find('/'):], buff)\n",
    "\n",
    "print(htmlf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name preffix removal check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob('json/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auts = []\n",
    "for fn in json_files:\n",
    "    with open(fn) as f:\n",
    "        dt = json.load(f)\n",
    "    \n",
    "    for el in dt:\n",
    "        if el['autor'] is not None:\n",
    "            auts.append(el['autor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Vojtěch Filip', 12595), ('Petr Gazdík', 8884), ('Jan Bartošek', 7854)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(auts).most_common()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names longer then three words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Augustin Karel Andrle Sylor',\n",
       " 'Markéta Pekarová Adamová',\n",
       " 'Jaroslava Pokorná Jermanová',\n",
       " 'Zuzana Majerová Zahradníková',\n",
       " 'Jana Mračková Vildumetzová',\n",
       " 'Tomáš Jan Podivínský',\n",
       " 'Zuzka Bebarová Rujbrová',\n",
       " 'Hana Aulická Jírovcová']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[j for j in set(auts) if len(j.split(' ')) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenization\n",
    "\n",
    "Take all the text from jsons, lower-case it, remove all punctuation, and remove leading/trailing whitespace\n",
    "\n",
    "Tokenize the sentences and write them to a file where each line is one sentence\n",
    "\n",
    "Sentences containing numeric characters are excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob('json/*.json')\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "str_builder = ''\n",
    "\n",
    "file = open('./vocabulary.txt', 'w+')\n",
    "\n",
    "for fn in json_files:\n",
    "    with open(fn) as f:\n",
    "        jsf = json.load(f)\n",
    "    \n",
    "    for el in jsf:\n",
    "        for s in sent_tokenize(el['text']):\n",
    "            processed_str = s.lower().translate(translator).strip()\n",
    "            if processed_str and not hasNumbers(processed_str):\n",
    "                str_builder += processed_str + '\\n'\n",
    "    file.write(str_builder)\n",
    "    str_builder = ''\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
