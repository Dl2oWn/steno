{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stenoprotokoly - příprava dat\n",
    "Web poslanecké sněmovny má velký archív stenoprotokolů. Můžete si třeba pročíst, jak se probíraly záležitosti [v roce 1920](http://www.psp.cz/eknih/1920ns/index.htm). Nás bude zajímat současná sněmovna, k níž stena najdeme [tady](http://www.psp.cz/eknih/2013ps/stenprot/index.htm). Naštěstí nemusíme data stahovat přímo v tomto formátu, PSP nabízí i zazipovaný archív, konkrétně [tady](http://www.psp.cz/eknih/2013ps/stenprot/zip/index.htm).\n",
    "\n",
    "Probíhat to bude asi takto: stáhneme zipy, rozbalíme, rozparsujeme do jsonů, nasypeme do skladiště dle libosti. Bude třeba Python 3 a pár balíků. Většina ve standardní knihovně, mimo ní používám akorat PyQuery.\n",
    "\n",
    "Je třeba dodělat:\n",
    "- datum projevu\n",
    "- rozparsovat jméno řečníka (prozatím funkční)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as ur\n",
    "import urllib.parse as up\n",
    "import urllib\n",
    "# from pyquery import PyQuery as pq\n",
    "import lxml.html\n",
    "import os.path\n",
    "import zipfile\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'http://www.psp.cz/eknih/'\n",
    "zp_folder = './zip/'\n",
    "zp_ext = '.zip'\n",
    "st_folder = './html/'\n",
    "js_folder = 'json/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stáhneme stránku se seznamem jednání a najdeme odkazy na jednání Poslanecké sněmovny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = lxml.html.parse(base).getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.psp.cz/eknih/2017ps/index.htm', 'http://www.psp.cz/eknih/2013ps/index.htm', 'http://www.psp.cz/eknih/2010ps/index.htm', 'http://www.psp.cz/eknih/2006ps/index.htm', 'http://www.psp.cz/eknih/2002ps/index.htm', 'http://www.psp.cz/eknih/1998ps/index.htm', 'http://www.psp.cz/eknih/1996ps/index.htm']\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "\n",
    "libs = ht.cssselect('div#main-content')[0]\n",
    "for elem_a, elem_b in zip(libs.cssselect('a'), libs.cssselect('a b')):\n",
    "    if elem_b.text.endswith('Poslanecká sněmovna'):\n",
    "        links.append(up.urljoin(base, elem_a.attrib['href']))\n",
    "\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ze stránek jednotlivých jednání PS vezmeme odkazy na komprimované stenografy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.psp.cz/eknih/2017ps/stenprot/zip/', 'http://www.psp.cz/eknih/2013ps/stenprot/zip/index.htm', 'http://www.psp.cz/eknih/2010ps/stenprot/zip/index.htm', 'http://www.psp.cz/eknih/2006ps/stenprot/zip/index.htm', 'http://www.psp.cz/eknih/2002ps/stenprot/zip/index.htm']\n"
     ]
    }
   ],
   "source": [
    "comp_links = []\n",
    "\n",
    "for ln in links:\n",
    "    page = lxml.html.parse(ln).getroot().cssselect('div#main-content')[0]\n",
    "    for elem_a in page.cssselect('a'):\n",
    "        if elem_a.text:\n",
    "            if 'komprimované' in elem_a.text:\n",
    "                comp_links.append(up.urljoin(ln, elem_a.attrib['href']))\n",
    "\n",
    "print(comp_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stenoprotokoly jsou zazipované, ale jsou stále ve spousta balících. Stáhneme tedy stránku, na které jsou, projdeme odkazy na zipy a stáhneme ty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://www.psp.cz/eknih/2017ps/stenprot/zip/037schuz.zip done.\n",
      "Retrieving http://www.psp.cz/eknih/2013ps/stenprot/zip/059schuz.zip failed: HTTP Error 404: Not Found\n",
      "Retrieving http://www.psp.cz/eknih/2013ps/stenprot/zip/060schuz.zip failed: HTTP Error 404: Not Found\n",
      "Retrieving http://www.psp.cz/eknih/2013ps/stenprot/zip/061schuz.zip failed: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('zip'): os.mkdir('zip')\n",
    "page_num = 0\n",
    "for comp_link in comp_links:\n",
    "    page = lxml.html.parse(comp_link).getroot()\n",
    "    for ln in page.cssselect('div#main-content a'):\n",
    "        ln_attr_href = ln.attrib['href'].split('/')[-1]\n",
    "        target = os.path.join(zp_folder, str(page_num) + '_' + ln_attr_href)\n",
    "        if os.path.isfile(target): continue\n",
    "        try:\n",
    "            print('Retrieving ' + up.urljoin(comp_link, ln_attr_href) + ' ', end='')\n",
    "            ur.urlretrieve(up.urljoin(comp_link, ln_attr_href), target)\n",
    "            print('done.')\n",
    "        except urllib.error.HTTPError as err:\n",
    "            print('failed: {}'.format(err))\n",
    "    page_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozbalíme (šlo by číst přímo z archivů, ale proč si to komplikovat, když to místa zabírá málo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./zip/2_042schuz.zip to ./html/2_042schuz failed: [Errno 22] Invalid argument\n",
      "Extracting ./zip/0_037schuz.zip to ./html/0_037schuz done.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('html'): os.mkdir('html')\n",
    "\n",
    "for arch in os.listdir(zp_folder):\n",
    "    if arch.endswith(zp_ext):\n",
    "        file_name = os.path.join(zp_folder, arch)\n",
    "        target = os.path.join(st_folder, arch.split('.')[0])\n",
    "\n",
    "        if os.path.exists(target): continue\n",
    "        \n",
    "        with zipfile.ZipFile(file_name) as zpf:\n",
    "            print('Extracting ' + file_name + ' to ' + target + ' ', end='')\n",
    "            try:\n",
    "                zpf.extractall(target)\n",
    "                print('done.')\n",
    "            except OSError as err:\n",
    "                os.rmdir(target)\n",
    "                print('failed: {}'.format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Převedeme do textu. Není tu moc rocket science, vesměs jde o:\n",
    "\n",
    "- Každý odstavec je buďto pokračování projevu, nebo začátek nového.\n",
    "- Začátek projevu poznáme tak, že začíná odkazem na profil řečníka. Hledat odkaz v textu je ale celkem riskantní, protože některé projevy (zvlášť v minulé Sněmovně) obsahovaly odkazy na hlasování. Tohle bude třeba ještě nějak ošetřit.\n",
    "- Proslovy mohou přetékat mezi soubory. Na začátku souboru je tak třeba zjistit, zda jde o pokračování, nebo ne. Já to řeším tak, že konce souborů neřeším, celá jedna schůze je pro mě tok textu, bez ohledu na rozdělení mezi soubory.\n",
    "- Jména řečníků jsou vč. jejich funkcí. To je nepříjemné ze dvou důvodů: jméno je moc dlouhé, zvlášť u facetování (filtrace v hledání), ale hlavně protože jméno pak není unikátní, protože funkce se mění, zvlášť mezi sněmovnami. Když pak budeme chtít seskupit spoustu let dohromady, budeme mít např. Bohuslava Sobotku jako ministra financí, poslance a premiéra. Bohužel rozdělení jména na funkce a vlastní jméno není úplně jednoznačné, bude to chtít vypsat počet variací a ošetřit to.\n",
    "- EDIT: Pro současná dostupná data (říjen 2019) odstranení pozic funguje docela dobře. Stačí do proměnné \"poz\" vyjmenovat slova, kterými pozice končí a podle nic odkrojit začátek stringu. Problém nastane, až se některý z politiků bude jmenovat \"Poslanec\", \"Senátor\" atp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(js_folder): os.mkdir(js_folder)\n",
    "html_files = glob.glob('./html/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "poz = 'Poslanec; PSP; Paní; Senátorka; Senátor; \\\n",
    "Poslankyně; mužů; Předsedající; práv; republiky; financí; prostředí; věcí; ČR'.split('; ')\n",
    "\n",
    "def rm_position(name):    \n",
    "    for p in poz:\n",
    "        if p in name:\n",
    "            name = name[name.rindex(p) + len(p) + 1:]\n",
    "            \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./html/2_010schuz\n"
     ]
    }
   ],
   "source": [
    "def write_json(nm, dt):\n",
    "    fn = os.path.join(js_folder, '%s.json' % nm)\n",
    "    \n",
    "    with open(fn, 'w') as f:\n",
    "        t = json.dump(dt, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "res = []\n",
    "pid = 0\n",
    "aut = None\n",
    "tema = None\n",
    "buf = []\n",
    "for htmlf in html_files:\n",
    "    buff = []\n",
    "    fns = glob.glob(os.path.join(htmlf, 's*.htm'))\n",
    "    for fn in fns:\n",
    "        h = lxml.html.parse(fn).getroot()\n",
    "        for p in h.cssselect('p'):\n",
    "            pt = p.text_content().strip()\n",
    "            if len(pt) == 0: continue\n",
    "            pt = pt.replace('\\xa0', ' ')\n",
    "            \n",
    "            od = p.find('a') # v textu je odkaz\n",
    "            if od is None:\n",
    "                buf += [pt]\n",
    "                continue\n",
    "\n",
    "            if len(buf) > 0:\n",
    "                buff.extend([OrderedDict(id=pid, autor=aut, schuze=int(htmlf.split('/')[-1][:3]),\\\n",
    "                                         fn=fn, tema=tema, text='\\n'.join(buf))])\n",
    "\n",
    "            aut = rm_position(od.text.strip())\n",
    "            buf = [pt[len(od.text)+1:].strip()] # pridame soucasny text (ale odseknem autora)\n",
    "            pid += 1\n",
    "    write_json(htmlf[-htmlf[::-1].find('/'):], buff)\n",
    "\n",
    "print(htmlf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontrola odstranení pozic ze jmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob('json/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auts = []\n",
    "for fn in json_files:\n",
    "    with open(fn) as f:\n",
    "        dt = json.load(f)\n",
    "    \n",
    "    for el in dt:\n",
    "        if el['autor'] is not None:\n",
    "            auts.append(el['autor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nejčastější mluvčí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Vojtěch Filip', 12595), ('Petr Gazdík', 8884), ('Jan Bartošek', 7854)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(auts).most_common()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jména delší než dvě slova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Augustin Karel Andrle Sylor',\n",
       " 'Markéta Pekarová Adamová',\n",
       " 'Jaroslava Pokorná Jermanová',\n",
       " 'Zuzana Majerová Zahradníková',\n",
       " 'Jana Mračková Vildumetzová',\n",
       " 'Tomáš Jan Podivínský',\n",
       " 'Zuzka Bebarová Rujbrová',\n",
       " 'Hana Aulická Jírovcová']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[j for j in set(auts) if len(j.split(' ')) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Větná tokenizace\n",
    "\n",
    "Z json souborů vezmeme všechen text, změníme na malá písmena, odstraníme veškerou diakritiku a bílé znaky na začátku/konci vět.\n",
    "\n",
    "Věty pak zapíšeme do souboru, kde každý řádek bude právě jedna věta.\n",
    "\n",
    "Do souboru nezapisuji věty obsahující číslice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = glob.glob('json/*.json')\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "str_builder = ''\n",
    "\n",
    "file = open('./vocabulary.txt', 'w+')\n",
    "\n",
    "for fn in json_files:\n",
    "    with open(fn) as f:\n",
    "        jsf = json.load(f)\n",
    "    \n",
    "    for el in jsf:\n",
    "        for s in sent_tokenize(el['text']):\n",
    "            processed_str = s.lower().translate(translator).strip()\n",
    "            if processed_str and not hasNumbers(processed_str):\n",
    "                str_builder += processed_str + '\\n'\n",
    "    file.write(str_builder)\n",
    "    str_builder = ''\n",
    "\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
